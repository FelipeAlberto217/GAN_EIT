# -*- coding: utf-8 -*-
"""GAN_EIT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17q12rVimycxcc4GzK7jRV8manB74y5eB
"""

import torch
print(torch.__version__)
print("GPU Available: ", torch.cuda.is_available())

if torch.cuda.is_available():   #GPU details
  device=torch.device("cuda:0")
else:
  device="cpu"
print(device)

from google.colab import drive #To use images located at Google Drive folder
drive.mount('/content/drive/')

import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt

#Generator function definition:
def make_generator_network(
    input_size=128,
    num_hidden_layers=1,
    num_hidden_units=12500,
    num_output_units=16384):
  model=nn.Sequential()
  for i in range(num_hidden_layers):
    model.add_module(f'fc_g{i}',
                     nn.Linear(input_size, num_hidden_units))
    model.add_module(f'relu_g{i}',nn.LeakyReLU())
    input_size=num_hidden_units
  model.add_module(f'fc_g{num_hidden_layers}',
                   nn.Linear(input_size, num_output_units))
  model.add_module('tanh_g', nn.Tanh())
  return model

#Discriminator function definition:
def make_discriminator_network(
    input_size=128,
    num_hidden_layers=1,
    num_hidden_units=12500,
    num_output_units=1):
  model=nn.Sequential()
  for i in range(num_hidden_layers):
    model.add_module(
        f'fc_d{i}',
        nn.Linear(input_size, num_hidden_units,bias=False)
    )
    model.add_module(f'relu_d{i}',nn.LeakyReLU())
    model.add_module('dropout',nn.Dropout(p=0.5))
    input_size=num_hidden_units
  model.add_module(f'fc_g{num_hidden_layers}',
                   nn.Linear(input_size, num_output_units))
  model.add_module('sigmoid', nn.Sigmoid())
  return model

##Definition of training settings for model
##generator model
image_size=(128,128)
z_size=128
gen_hidden_layers=1
gen_hidden_size=12500
disc_hidden_layers=1
disc_hidden_size=12500
torch.manual_seed(1)
##generator model
gen_model=make_generator_network(
    input_size=z_size,
    num_hidden_layers=gen_hidden_layers,
    num_hidden_units=gen_hidden_size,
    num_output_units=np.prod(image_size)
)
print(gen_model)

##Discriminator model
disc_model=make_discriminator_network(
    input_size=np.prod(image_size),
    num_hidden_layers=disc_hidden_layers,
    num_hidden_units=disc_hidden_size
)
print(disc_model)

#Defining training data set (rho images)
import pandas as pd
import numpy
import torchvision                                                                      #To preprocess data for GAN input
from torchvision import transforms

transform=transforms.Compose([                                                          #Transformations for preprocessing
    transforms.ToTensor(),
])

first=True
for i in range(0,30131):
  df=pd.read_csv("path_to_target_data"+str(i+1)+".csv")
  df_numpy=df.to_numpy()
  norm_data=transform(df_numpy)
  if first is True:
    norm_data_total=norm_data
    first=False
  else:
    norm_data_total=torch.cat((norm_data_total,norm_data),0)

norm_data=transform(df_numpy)
example=next(iter(norm_data))
print(f'Min:{norm_data.min()} Max:{norm_data.max()}')
print(norm_data_total.data.shape)

#Defining input signal for generator("noise")
import random
def read_input(batch_size):
  transform=transforms.Compose([                                                          #Transformations for preprocessing
    transforms.ToTensor(),
  ])

  for i in range(batch_size):    #for i in range(batch_size):
    voltage_number=random.randint(0,30131)
    df=pd.read_csv("path_to_voltage_data"+str(voltage_number+1)+".csv")
    df_numpy=df.to_numpy()
    norm_data=transform(df_numpy)

    if i is 0:
      input_z=norm_data
    else:                 #else
      input_z=torch.cat((input_z,norm_data),0)

  input_z=input_z[:,:,0]
  input_z.float()

  return input_z

#Ground truth labels
loss_fn=nn.BCELoss()
  ##Loss for the Generator
g_labels_real=torch.ones_like(d_proba_fake)
g_loss=loss_fn(d_proba_fake, g_labels_real)
print(f'Generator Loss: {g_loss:.4f}')

  ##Loss for the Discriminator
d_labels_real=torch.ones_like(d_proba_real)
d_labels_fake=torch.zeros_like(d_proba_fake)
d_loss_real=loss_fn(d_proba_real,d_labels_real)
d_loss_fake=loss_fn(d_proba_fake,d_labels_fake)
print(f'Discriminator Losses: Real {d_loss_real:.4f} Fake {d_loss_fake:.4f}')

#Setting up the GAN model
batch_size=16
torch.manual_seed(1)
np.random.seed(1)
print(norm_data_total.data.size())
mnist_dl=DataLoader(norm_data_total,batch_size=batch_size,   #Data loader for real dataset
                    shuffle=True,drop_last=True)
gen_model=make_generator_network(                            #Setting up of generator model
    input_size=z_size,
    num_hidden_layers=gen_hidden_layers,
    num_hidden_units=gen_hidden_size,
    num_output_units=np.prod(image_size)
).to(device)
disc_model=make_discriminator_network(                       #Setting up of discriminator model
    input_size=np.prod(image_size),
    num_hidden_layers=disc_hidden_layers,
    num_hidden_units=disc_hidden_size,
).to(device)

loss_fn=nn.BCELoss()                                         #Setting up of Adam optimizers for each model
g_optimizer=torch.optim.Adam(gen_model.parameters(),lr=0.002,betas=(0.5,0.999))
d_optimizer=torch.optim.Adam(disc_model.parameters(),lr=0.002,betas=(0.5,0.999))
#Training of the discriminator
def d_train(x):
  disc_model.zero_grad()
  #Train discriminator with a real batch
  batch_size=x.size(0)
  x=x.view(batch_size,-1).to(device)
  d_labels_real=torch.ones(batch_size,1,device=device)
  d_proba_real=disc_model(x)
  d_loss_real=loss_fn(d_proba_real,d_labels_real)
  #Train discriminator on a fake batch
  input_z=read_input(batch_size).to(device)
  input_z=input_z.float()
  g_output=gen_model(input_z)
  d_proba_fake=disc_model(g_output)
  d_labels_fake=torch.zeros(batch_size,1,device=device)
  d_loss_fake=loss_fn(d_proba_fake,d_labels_fake)
  #Gradient backpropagation and optimization of only discriminator's parameters
  d_loss=d_loss_real+d_loss_fake
  d_loss.backward()
  d_optimizer.step()
  return d_loss.data.item(),d_proba_real.detach(), d_proba_fake.detach()

  #Training of the generator
def g_train(x):
  gen_model.zero_grad()
  batch_size=x.size(0)
  input_z=read_input(batch_size).to(device)
  input_z=input_z.float()
  g_labels_real=torch.ones(batch_size,1,device=device)

  g_output=gen_model(input_z)
  d_proba_fake=disc_model(g_output)
  g_loss=loss_fn(d_proba_fake,g_labels_real)
  #Gradient backprop and optimize only generator's parameters
  g_loss.backward()
  g_optimizer.step()
  return g_loss.data.item()

fixed_z=read_input(batch_size).to(device)
def create_samples(g_model,input_z):
  input_z=input_z.float()
  g_output=g_model(input_z)
  images=torch.reshape(g_output, (batch_size,*image_size))
  return(images+1)/2.0

epoch_samples=[]
all_d_losses=[]
all_g_losses=[]
all_d_real=[]
all_d_fake=[]
num_epochs=200

for epoch in range(1, num_epochs+1):     #Alternation of training for generator and discriminator
  d_losses, g_losses=[],[]
  d_vals_real, d_vals_fake=[],[]
  for i, (x) in enumerate(mnist_dl):
    d_loss,d_proba_real,d_proba_fake=d_train(x.type(torch.FloatTensor))
    d_losses.append(d_loss)
    g_losses.append(g_train(x))
    d_vals_real.append(d_proba_real.mean().cpu())
    d_vals_fake.append(d_proba_fake.mean().cpu())


  all_d_losses.append(torch.tensor(d_losses).mean())
  all_g_losses.append(torch.tensor(g_losses).mean())
  all_d_real.append(torch.tensor(d_vals_real).mean())
  all_d_fake.append(torch.tensor(d_vals_fake).mean())
  print(f'Epoch {epoch:03d} | Avg Losses >>'
    f' G/D {all_g_losses[-1]:.4f}/{all_d_losses[-1]:.4f}'
    f' [D-Real: {all_d_real[-1]:.4f}'
    f' D-Fake: {all_d_fake[-1]:.4f}]')                            #Loss for generator, discriminator and real data and fake data

  epoch_samples.append(
      create_samples(gen_model, fixed_z).detach().cpu().numpy()
  )                                                               #Saves the created samples according to the epochs

#Plot of average probabilities
import itertools
fig=plt.figure(figsize=(16,6))
##Plotting the losses
ax=fig.add_subplot(1,2,1)
plt.plot(all_g_losses, label='Generator loss')
half_d_losses=[all_d_loss/2 for all_d_loss in all_d_losses]
plt.plot(half_d_losses, label='Discriminator loss')
plt.legend(fontsize=20)
ax.set_xlabel('Iteration', size=15)
ax.set_ylabel('Loss', size=15)

##Plotting the outputs of the discriminator
ax=fig.add_subplot(1,2,2)
plt.plot(all_d_real,label=r'Real: $D(\mathbf{x})$')
plt.plot(all_d_fake, label=r'Fake: $D(G(\mathbf{z}))$')
plt.legend(fontsize=20)
ax.set_xlabel('Iteration', size=15)
plt.show

#Output of the generator(synthesized images)
selected_epochs=[1,50,100,150,200]
fig=plt.figure(figsize=(18,18))        #(6,166)
for i,e in enumerate(selected_epochs):
  for j in range(5):
    ax=fig.add_subplot(6,5,i*5+j+1)    #(6,5,i*5+j+1)
    ax.set_xticks([])
    ax.set_yticks([])
    if j==0:
      ax.text(
          -0.06,0.5,f'Epoch {e}',
          rotation=90,size=18,color='red',
          horizontalalignment='right',
          verticalalignment='center',
          transform=ax.transAxes
      )
    image=epoch_samples[e-1][j]   #[e-1][j]
    ax.imshow(image,cmap='viridis')
plt.show()

#Output to csv file
from pathlib import Path
filepath = Path('/content/drive/MyDrive/out.csv')
df = pd.DataFrame(epoch_samples[4][1])
df.to_csv(filepath)

"""## Review of validation cases"""

#Defining input
def read_input_validation(batch_size):
  transform=transforms.Compose([                                                          #Transformations for preprocessing
    transforms.ToTensor(),
  ])


  for i in range(batch_size):
    df=pd.read_csv("path_to_voltages_for_validation_cases"+str(i+50)+".csv")
    df_numpy=df.to_numpy()
    norm_data=transform(df_numpy)

    if i is 0:
      input_z=norm_data
    else:
      input_z=torch.cat((input_z,norm_data),0)


  input_z=input_z[:,:,0]
  input_z.float()

  return input_z

new_z=read_input_new(batch_size).to(device)
new_z.size()

epoch_new=[]
epoch_new.append(
      create_samples(gen_model, new_z).detach().cpu().numpy())

#Output of the generator(synthesized images)
selected_epochs=[1,10,50,100,200]
fig=plt.figure(figsize=(18,18))
for i,e in enumerate(selected_epochs):
  for j in range(1):
    ax=fig.add_subplot(6,5,i*5+j+1)
    ax.set_xticks([])
    ax.set_yticks([])
    if j==0:
      ax.text(
          -0.06,0.5,f'Epoch {e}',
          rotation=90,size=18,color='red',
          horizontalalignment='right',
          verticalalignment='center',
          transform=ax.transAxes
      )
    image=epoch_new[e-1][j]   #[e-1][j]
    ax.imshow(image,cmap='viridis')
plt.show()
